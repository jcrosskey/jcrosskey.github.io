---
layout: post
title:  "Gradient Descent in Neural Network Training"
date:   Fri Apr 21 10:16:52 EDT 2017
categories: optimization, neural network
---
In this post I summarize what I have learned about optimization methods and tricks 
in training neural network.
* TOC
{:toc}

### Normalization of input before affine transformation

### Initialization of weights and biases

### Optimization, Adaptive learning rate, and Annealing schedule

#### Variance-based SGD (vSGD)

### References
[1] [No More Pesky Learning Rates][LeCun2012] By Tom Schaul, Sixin Zhang, and Yann LeCun

[LeCun2012]: https://arxiv.org/pdf/1206.1106.pdf

